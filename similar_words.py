# -*- coding: utf-8 -*-
"""similar_words

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pBXC0PvrKhKjjVw4tVPc_mdooaDhzmw_
"""

from nltk.stem import WordNetLemmatizer
from collections import defaultdict
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
import nltk
from collections import defaultdict

ps = PorterStemmer()

words = ["crime", "crimes", "Crime", "murders", "homicides", "criminality", "criminals", "homicide", "Crimes", "hate_crimes", "Violent_crime",
         "drug_trafficking", "Violent_crimes", "cybercrime", "violence", "Homicides", "murder", "gangland_killings", "Violent_Crime", "cybercrime"]



def remove_similar_words(words):

    distancedict = defaultdict(set)
    for idx, w in enumerate(words):
        w = w.replace("_", " ")
        #print(w, " : ", ps.stem(w))
        wordstem = ps.stem(w)
        #distancedict = {defaultdict: list}
        for jdx in range(idx, len(words)):
            distance = nltk.edit_distance(
                words[idx], words[jdx], transpositions=False)
            # print(distance)
            if distance <= 2:
                distancedict[wordstem].add(words[jdx])

    # print(dict(distancedict))
    for key in distancedict:
        print(key, distancedict[key])


    nltk.download('wordnet')
    lemmatizer = WordNetLemmatizer()

    distancedict = defaultdict(set)
    merged = defaultdict(lambda: False)
    distancedictFinal = defaultdict(set)



def getWordLemma():
    for w in words:
        wlower = w.lower()
        #w=w.replace("_", " ")
        if "_" not in w:
            #print(w, " : ", lemmatizer.lemmatize(w))
            wordlemma = lemmatizer.lemmatize(wlower)

        else:
            w_list = w.split("_")
            # print(w_list)
            w_newlist = []
            for singlew in w_list:
                #print(singlew, " : ", lemmatizer.lemmatize(singlew))
                singlew = singlew.lower()
                w_newlist.append(lemmatizer.lemmatize(singlew))

            wordlemma = ' '.join(w_newlist)

        distancedict[wordlemma].add(w)

# the distance is calculated after the lemmatization


def calculateLemmaDistance():
    for key1 in distancedict:
        for key2 in distancedict:
            if key1 != key2:
                distance = nltk.edit_distance(key1, key2, transpositions=False)
                # print(distance)
                #print(key1, key2, distance)
                #if key1 == "gangland killing": print(key1, key2, distance)
                if distance <= 3 and merged[key2] == False:
                    merged[key1] = True
                    merged[key2] = True
                    newSet = distancedictFinal[key1]
                    newSet.update(distancedict[key1])
                    newSet.update(distancedict[key2])
                    distancedictFinal[key1] = newSet

                    #print(key1, key2)
                    # print(distancedict[key1])
                    #distancedictFinal[key1] = defaultdict[key1].union(defaultdict[key2])


def processMergedFalse():
    for key in distancedict:
        #print(key, distancedict[key])
        if merged[key] == False:
            distancedictFinal[key] = distancedict[key]


getWordLemma()
calculateLemmaDistance()
processMergedFalse()


for key in sorted(distancedictFinal.keys()):
    print(key, distancedictFinal[key])
